{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# Cell 2: Ornstein-Uhlenbeck Noise\n",
    "class OrnsteinUhlenbeckNoise:\n",
    "    def __init__(self, mu, theta=0.15, sigma=0.2):\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.zeros_like(mu)\n",
    "\n",
    "    def __call__(self):\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(*self.mu.shape)\n",
    "        self.state += dx\n",
    "        return self.state\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros_like(self.mu)\n",
    "\n",
    "# Cell 3: Actor and Critic Networks\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Cell 4: Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "# Cell 5: MADDPG Agent\n",
    "class MADDPGAgent:\n",
    "    def __init__(self, obs_dim, act_dim, num_agents=2):\n",
    "        self.num_agents = num_agents\n",
    "        self.actors = [Actor(obs_dim, act_dim) for _ in range(num_agents)]\n",
    "        self.critics = [Critic(obs_dim * num_agents + act_dim * num_agents) for _ in range(num_agents)]\n",
    "        self.target_actors = [copy.deepcopy(actor) for actor in self.actors]\n",
    "        self.target_critics = [copy.deepcopy(critic) for critic in self.critics]\n",
    "\n",
    "        self.actor_opts = [optim.Adam(actor.parameters(), lr=1e-4) for actor in self.actors]\n",
    "        self.critic_opts = [optim.Adam(critic.parameters(), lr=1e-3) for critic in self.critics]\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        self.tau = 0.01\n",
    "        self.gamma = 0.99\n",
    "        self.noises = [OrnsteinUhlenbeckNoise(mu=np.zeros(act_dim)) for _ in range(num_agents)]\n",
    "\n",
    "    def select_action(self, obs_list):\n",
    "        with torch.no_grad():\n",
    "            actions = []\n",
    "            for i, (actor, obs) in enumerate(zip(self.actors, obs_list)):\n",
    "                obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
    "                action = actor(obs_tensor).numpy() + self.noises[i]().astype(np.float32)\n",
    "                actions.append(np.clip(action, -1, 1))\n",
    "            return actions\n",
    "\n",
    "    def update(self, batch_size=64):\n",
    "        if len(self.replay_buffer.buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        samples = self.replay_buffer.sample(batch_size)\n",
    "        obs_n = [torch.tensor([s[0][i] for s in samples], dtype=torch.float32) for i in range(self.num_agents)]\n",
    "        act_n = [torch.tensor([s[1][i] for s in samples], dtype=torch.float32) for i in range(self.num_agents)]\n",
    "        rew_n = [torch.tensor([s[2][i] for s in samples], dtype=torch.float32).unsqueeze(1) for i in range(self.num_agents)]\n",
    "        next_obs_n = [torch.tensor([s[3][i] for s in samples], dtype=torch.float32) for i in range(self.num_agents)]\n",
    "\n",
    "        all_obs = torch.cat(obs_n, dim=1)\n",
    "        all_acts = torch.cat(act_n, dim=1)\n",
    "        next_acts = [self.target_actors[i](next_obs_n[i]) for i in range(self.num_agents)]\n",
    "        all_next_obs = torch.cat(next_obs_n, dim=1)\n",
    "        all_next_acts = torch.cat(next_acts, dim=1)\n",
    "\n",
    "        for i in range(self.num_agents):\n",
    "            target_q = self.target_critics[i](torch.cat([all_next_obs, all_next_acts], dim=1)).detach()\n",
    "            current_q = self.critics[i](torch.cat([all_obs, all_acts], dim=1))\n",
    "            critic_loss = nn.MSELoss()(current_q, rew_n[i] + self.gamma * target_q)\n",
    "\n",
    "            self.critic_opts[i].zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_opts[i].step()\n",
    "\n",
    "            new_act = self.actors[i](obs_n[i])\n",
    "            all_new_acts = act_n.copy()\n",
    "            all_new_acts[i] = new_act\n",
    "            actor_input = torch.cat(obs_n, dim=1)\n",
    "            new_actions = torch.cat(all_new_acts, dim=1)\n",
    "            actor_loss = -self.critics[i](torch.cat([actor_input, new_actions], dim=1)).mean()\n",
    "\n",
    "            self.actor_opts[i].zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_opts[i].step()\n",
    "\n",
    "            for target_param, param in zip(self.target_critics[i].parameters(), self.critics[i].parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for target_param, param in zip(self.target_actors[i].parameters(), self.actors[i].parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "# Cell 6: Budgeting Environment\n",
    "class BudgetingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.num_agents = 2\n",
    "        self.max_days = 30\n",
    "        self.agents = ['budgeter', 'impulse_buyer']\n",
    "        self.observation_space = spaces.Box(low=0, high=1000, shape=(7,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.balances = {'budgeter': 1000.0, 'impulse_buyer': 1000.0}\n",
    "        self.happiness = {'budgeter': 50.0, 'impulse_buyer': 50.0}\n",
    "        self.day = 0\n",
    "        self.history = {agent: {'balance': [], 'happiness': []} for agent in self.agents}\n",
    "        return {agent: self._get_obs(agent) for agent in self.agents}\n",
    "\n",
    "    def step(self, actions):\n",
    "        rewards = {}\n",
    "        obs = {}\n",
    "        dones = {}\n",
    "\n",
    "        for agent in self.agents:\n",
    "            spend = actions[agent][0] * 100\n",
    "            self.balances[agent] -= spend\n",
    "            self.happiness[agent] += spend * random.uniform(0.05, 0.2)\n",
    "            self.happiness[agent] = min(100, max(0, self.happiness[agent]))\n",
    "\n",
    "            penalty = (1000 - self.balances[agent]) * 0.01\n",
    "            reward = self.happiness[agent] - 50 - penalty + 0.1 * (self.balances[agent] / 1000)\n",
    "            rewards[agent] = reward\n",
    "            obs[agent] = self._get_obs(agent)\n",
    "            dones[agent] = self.day >= self.max_days - 1\n",
    "\n",
    "            self.history[agent]['balance'].append(self.balances[agent])\n",
    "            self.history[agent]['happiness'].append(self.happiness[agent])\n",
    "\n",
    "        self.day += 1\n",
    "        return obs, rewards, dones, {}\n",
    "\n",
    "    def _get_obs(self, agent):\n",
    "        return np.array([\n",
    "            self.balances[agent],\n",
    "            self.happiness[agent],\n",
    "            random.uniform(10, 50),\n",
    "            random.uniform(5, 30),\n",
    "            random.uniform(10, 40),\n",
    "            random.uniform(20, 80),\n",
    "            self.day\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "# Cell 7: Training + Visualization\n",
    "env = BudgetingEnv()\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "agent = MADDPGAgent(obs_dim, act_dim)\n",
    "\n",
    "episode_rewards = []\n",
    "all_results = []\n",
    "\n",
    "for episode in range(30):\n",
    "    obs_dict = env.reset()\n",
    "    obs_list = [obs_dict[agent_name] for agent_name in env.agents]\n",
    "    total_rewards = [0 for _ in range(env.num_agents)]\n",
    "\n",
    "    for step in range(env.max_days):\n",
    "        actions_list = agent.select_action(obs_list)\n",
    "        actions_dict = {agent_name: actions_list[i] for i, agent_name in enumerate(env.agents)}\n",
    "        next_obs_dict, reward_dict, done_dict, _ = env.step(actions_dict)\n",
    "        next_obs_list = [next_obs_dict[agent_name] for agent_name in env.agents]\n",
    "        reward_list = [reward_dict[agent_name] for agent_name in env.agents]\n",
    "        agent.replay_buffer.push((obs_list, actions_list, reward_list, next_obs_list))\n",
    "        agent.update()\n",
    "        obs_list = next_obs_list\n",
    "        for i in range(env.num_agents):\n",
    "            total_rewards[i] += reward_list[i]\n",
    "\n",
    "    episode_rewards.append(total_rewards)\n",
    "    all_results.append({'Episode': episode+1, 'Budgeter': total_rewards[0], 'Impulse Buyer': total_rewards[1]})\n",
    "\n",
    "# Graphs\n",
    "plt.figure(figsize=(10, 5))\n",
    "for agent in env.agents:\n",
    "    plt.plot(range(1, env.max_days + 1), env.history[agent]['happiness'], label=agent)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Happiness\")\n",
    "plt.title(\"Happiness over Days\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for agent in env.agents:\n",
    "    plt.plot(range(1, env.max_days + 1), env.history[agent]['balance'], label=agent)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Balance\")\n",
    "plt.title(\"Balance over Days\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot([r[0] for r in episode_rewards], label=\"Budgeter\")\n",
    "plt.plot([r[1] for r in episode_rewards], label=\"Impulse Buyer\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Rewards per Episode\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "reward_df = pd.DataFrame(all_results)\n",
    "print(reward_df)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
